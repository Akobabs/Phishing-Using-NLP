{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69f8fce",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) for Phishing Detection\n",
    "\n",
    "This notebook performs EDA on three datasets: Enron (`enron_spam_data.csv`), PhishTank (`phishtank_data.csv`), and UCI Phishing Websites (`Training_Dataset.arff`). The goal is to understand the data, identify patterns, and prepare it for a BERT-based phishing detection system.\n",
    "\n",
    "## Objectives\n",
    "- Understand dataset structure (size, columns, missing values, duplicates).\n",
    "- Analyze label distribution (phishing vs. legitimate).\n",
    "- Explore text characteristics (length, word frequency, bigrams, trigrams, clusters) for Enron and PhishTank.\n",
    "- Analyze numerical features for UCI, including feature importance.\n",
    "- Combine text datasets (Enron + PhishTank), balance with SMOTE, and reassess.\n",
    "- Provide detailed preprocessing recommendations.\n",
    "\n",
    "## Datasets\n",
    "- **Enron**: ~33,716 emails, columns: `Message ID`, `Subject`, `Message`, `Spam/Ham`, `Date`.\n",
    "- **PhishTank**: ~64,753 phishing URLs, columns: `phish_id`, `url`, `phish_detail_url`, etc.\n",
    "- **UCI**: ~11,055 website records, 30 numerical features, `Result` (no URL).\n",
    "\n",
    "Outputs are saved to `results/eda/`. Intermediate datasets are saved to `data/intermediate/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c6be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to download NLTK resources: {e}\")\n",
    "\n",
    "# Set up directories\n",
    "os.makedirs('results/eda', exist_ok=True)\n",
    "os.makedirs('data/intermediate', exist_ok=True)\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5b86b",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Define functions for text cleaning, dataset loading, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5562e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, is_url=False):\n",
    "    \"\"\"Preprocess text for EDA. URLs retain more components than emails.\"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            logger.debug(f\"Non-string input: {type(text)}\")\n",
    "            return ''\n",
    "        text = text.lower()\n",
    "        if is_url:\n",
    "            text = re.sub(r'http[s]?://', '', text)\n",
    "            text = re.sub(r'[^a-zA-Z0-9\\\\-\\\\.\\\\_/]', '', text)\n",
    "        else:\n",
    "            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "            text = re.sub(r'[^a-zA-Z\\\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        cleaned = ' '.join(tokens)\n",
    "        if not cleaned and text:\n",
    "            logger.debug(f\"Cleaned text is empty for input: {text[:50]}\")\n",
    "        return cleaned\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning text: {e}\")\n",
    "        return ''\n",
    "\n",
    "def dataset_summary(df, name):\n",
    "    \"\"\"Summarize dataset structure.\"\"\"\n",
    "    try:\n",
    "        summary = {\n",
    "            'Size': df.shape[0],\n",
    "            'Columns': list(df.columns),\n",
    "            'Missing Values': df.isnull().sum().to_dict(),\n",
    "            'Duplicates': df.duplicated().sum()\n",
    "        }\n",
    "        logger.info(f\"{name} Summary: {summary}\")\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error summarizing {name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def plot_missing_values(df, name):\n",
    "    \"\"\"Plot missing values.\"\"\"\n",
    "    try:\n",
    "        missing = df.isnull().sum()\n",
    "        if missing.sum() == 0:\n",
    "            logger.info(f\"No missing values in {name}\")\n",
    "            return\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=missing.index, y=missing.values)\n",
    "        plt.title(f'Missing Values in {name}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/missing_values_{name.lower()}.png')\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error plotting missing values for {name}: {e}\")\n",
    "\n",
    "def label_distribution(df, name):\n",
    "    \"\"\"Analyze and plot label distribution.\"\"\"\n",
    "    try:\n",
    "        if 'label' not in df.columns or df.empty:\n",
    "            logger.warning(f\"No labels or empty DataFrame for {name}\")\n",
    "            return {}\n",
    "        label_counts = df['label'].value_counts()\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=label_counts.index.astype(str), y=label_counts.values)\n",
    "        plt.title(f'Label Distribution in {name}')\n",
    "        plt.xlabel('Label (0: Legitimate, 1: Phishing)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/label_distribution_{name.lower()}.png')\n",
    "        plt.close()\n",
    "        return label_counts.to_dict()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error plotting label distribution for {name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def text_analysis(df, name):\n",
    "    \"\"\"Analyze text characteristics, including trigrams and clustering.\"\"\"\n",
    "    try:\n",
    "        if 'text' not in df.columns or df['text'].str.strip().eq('').all():\n",
    "            logger.warning(f\"No valid text data for {name}\")\n",
    "            return {}, [], [], [], [], [], []\n",
    "        df['char_length'] = df['text'].apply(len)\n",
    "        df['word_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        # Statistics\n",
    "        stats = {\n",
    "            'Char Length Mean': df['char_length'].mean(),\n",
    "            'Char Length Median': df['char_length'].median(),\n",
    "            'Word Length Mean': df['word_length'].mean(),\n",
    "            'Word Length Median': df['word_length'].median()\n",
    "        }\n",
    "        logger.info(f\"{name} Text Stats: {stats}\")\n",
    "\n",
    "        # Plot length distributions\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df['char_length'], bins=50)\n",
    "        plt.title(f'Character Length Distribution in {name}')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(df['word_length'], bins=50)\n",
    "        plt.title(f'Word Length Distribution in {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/text_length_{name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Word frequency\n",
    "        phishing_words = ' '.join(df[df['label'] == 1]['text']).split()\n",
    "        legit_words = ' '.join(df[df['label'] == 0]['text']).split()\n",
    "        phishing_freq = Counter(phishing_words).most_common(10)\n",
    "        legit_freq = Counter(legit_words).most_common(10)\n",
    "\n",
    "        # Plot word frequency\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if phishing_freq:\n",
    "            sns.barplot(x=[count for _, count in phishing_freq], y=[word for word, _ in phishing_freq])\n",
    "        plt.title(f'Top 10 Words in Phishing ({name})')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if legit_freq:\n",
    "            sns.barplot(x=[count for _, count in legit_freq], y=[word for word, _ in legit_freq])\n",
    "        plt.title(f'Top 10 Words in Legitimate ({name})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/word_freq_{name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Bigram frequency\n",
    "        phishing_bigrams = [bigram for text in df[df['label'] == 1]['text'] for bigram in ngrams(text.split(), 2)]\n",
    "        legit_bigrams = [bigram for text in df[df['label'] == 0]['text'] for bigram in ngrams(text.split(), 2)]\n",
    "        phishing_bigram_freq = Counter(phishing_bigrams).most_common(10)\n",
    "        legit_bigram_freq = Counter(legit_bigrams).most_common(10)\n",
    "\n",
    "        # Plot bigram frequency\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if phishing_bigram_freq:\n",
    "            sns.barplot(x=[count for _, count in phishing_bigram_freq], y=[' '.join(bigram) for bigram, _ in phishing_bigram_freq])\n",
    "        plt.title(f'Top 10 Bigrams in Phishing ({name})')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if legit_bigram_freq:\n",
    "            sns.barplot(x=[count for _, count in legit_bigram_freq], y=[' '.join(bigram) for bigram, _ in legit_bigram_freq])\n",
    "        plt.title(f'Top 10 Bigrams in Legitimate ({name})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/bigram_freq_{name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Trigram frequency\n",
    "        phishing_trigrams = [trigram for text in df[df['label'] == 1]['text'] for trigram in ngrams(text.split(), 3)]\n",
    "        legit_trigrams = [trigram for text in df[df['label'] == 0]['text'] for trigram in ngrams(text.split(), 3)]\n",
    "        phishing_trigram_freq = Counter(phishing_trigrams).most_common(10)\n",
    "        legit_trigram_freq = Counter(legit_trigrams).most_common(10)\n",
    "\n",
    "        # Plot trigram frequency\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if phishing_trigram_freq:\n",
    "            sns.barplot(x=[count for _, count in phishing_trigram_freq], y=[' '.join(trigram) for trigram, _ in phishing_trigram_freq])\n",
    "        plt.title(f'Top 10 Trigrams in Phishing ({name})')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if legit_trigram_freq:\n",
    "            sns.barplot(x=[count for _, count in legit_trigram_freq], y=[' '.join(trigram) for trigram, _ in legit_trigram_freq])\n",
    "        plt.title(f'Top 10 Trigrams in Legitimate ({name})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/trigram_freq_{name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Word clouds\n",
    "        if phishing_words:\n",
    "            phishing_cloud = WordCloud(width=800, height=400).generate(' '.join(phishing_words))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(phishing_cloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Phishing Word Cloud ({name})')\n",
    "            plt.savefig(f'results/eda/phishing_wordcloud_{name.lower()}.png')\n",
    "            plt.close()\n",
    "\n",
    "        if legit_words:\n",
    "            legit_cloud = WordCloud(width=800, height=400).generate(' '.join(legit_words))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(legit_cloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Legitimate Word Cloud ({name})')\n",
    "            plt.savefig(f'results/eda/legit_wordcloud_{name.lower()}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Text clustering\n",
    "        if not df['text'].str.strip().eq('').all():\n",
    "            vectorizer = TfidfVectorizer(max_features=1000)\n",
    "            X_text = vectorizer.fit_transform(df['text'])\n",
    "            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "            df['cluster'] = kmeans.fit_predict(X_text)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.countplot(x='cluster', hue='label', data=df)\n",
    "            plt.title(f'Text Cluster Distribution in {name}')\n",
    "            plt.legend(title='Label', labels=['Legitimate', 'Phishing'])\n",
    "            plt.savefig(f'results/eda/text_clusters_{name.lower()}.png')\n",
    "            plt.close()\n",
    "\n",
    "        return stats, phishing_freq, legit_freq, phishing_bigram_freq, legit_bigram_freq, phishing_trigram_freq, legit_trigram_freq\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in text analysis for {name}: {e}\")\n",
    "        return {}, [], [], [], [], [], []\n",
    "\n",
    "def uci_feature_analysis(df, name):\n",
    "    \"\"\"Analyze UCI numerical features with feature importance.\"\"\"\n",
    "    try:\n",
    "        feature_cols = [col for col in df.columns if col not in ['label']]\n",
    "        stats = {}\n",
    "        for col in feature_cols:\n",
    "            stats[col] = df[col].value_counts().to_dict()\n",
    "\n",
    "        # Plot feature distributions (first 9 features)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, col in enumerate(feature_cols[:9], 1):\n",
    "            plt.subplot(3, 3, i)\n",
    "            sns.countplot(x=col, hue='label', data=df)\n",
    "            plt.title(col)\n",
    "            plt.legend(title='Label', labels=['Legitimate', 'Phishing'])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'results/eda/feature_dist_{name.lower()}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Correlation with label\n",
    "        corr = df[feature_cols + ['label']].corr()\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
    "        plt.title(f'Feature Correlation in {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/feature_corr_{name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Feature importance\n",
    "        X = df[feature_cols]\n",
    "        y = df['label']\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        importance = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=importance.values, y=importance.index)\n",
    "        plt.title(f'Feature Importance in {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/eda/feature_importance_{name.lower()}.png')\n",
    "        plt.close()\n",
    "\n",
    "        return stats, corr, importance\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in UCI feature analysis: {e}\")\n",
    "        return {}, pd.DataFrame(), pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b8c43",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load and preprocess each dataset to a unified format with text and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a832a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:13:37,187 - INFO - Raw Enron Columns: ['Message ID', 'Subject', 'Message', 'Spam/Ham', 'Date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Enron Head:\n",
      "    Message ID                       Subject  \\\n",
      "0           0  christmas tree farm pictures   \n",
      "1           1      vastar resources , inc .   \n",
      "2           2  calpine daily gas nomination   \n",
      "3           3                    re : issue   \n",
      "4           4     meter 7268 nov allocation   \n",
      "\n",
      "                                             Message Spam/Ham        Date  \n",
      "0                                                NaN      ham  1999-12-10  \n",
      "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
      "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
      "3  fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
      "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
      "Raw Enron Missing Values:\n",
      " Message ID      0\n",
      "Subject       289\n",
      "Message       371\n",
      "Spam/Ham        0\n",
      "Date            0\n",
      "dtype: int64\n",
      "Raw Spam/Ham Unique Values:\n",
      " ['ham' 'spam']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:07,816 - INFO - Empty Text Count: 64\n",
      "2025-05-09 19:14:07,827 - INFO - Label Missing Values: 0\n",
      "2025-05-09 19:14:09,487 - INFO - Loaded Enron: 33652 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enron Sample:\n",
      "                                                text  label\n",
      "0                          christmastreefarmpictures      0\n",
      "1  vastarresourcesincgaryproductionfromthehighisl...      0\n",
      "2  calpinedailygasnominationcalpinedailygasnomina...      0\n",
      "3  reissuefyiseenotebelowalreadydonestellaforward...      0\n",
      "4  meternovallocationfyiforwardedbylauriaallenhou...      0\n",
      "Raw PhishTank Head:\n",
      "    phish_id                                    url  \\\n",
      "0   9057481  https://bayareafastrak.org-etcsw.win/   \n",
      "1   9057480  https://bayareafastrak.org-etcsv.win/   \n",
      "2   9057479  https://bayareafastrak.org-etcst.win/   \n",
      "3   9057478  https://bayareafastrak.org-etcsr.win/   \n",
      "4   9057477  https://bayareafastrak.org-etcsq.win/   \n",
      "\n",
      "                                    phish_detail_url  \\\n",
      "0  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "1  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "2  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "3  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "4  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "\n",
      "             submission_time verified          verification_time online target  \n",
      "0  2025-04-11T07:43:02+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "1  2025-04-11T07:42:49+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "2  2025-04-11T07:42:35+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "3  2025-04-11T07:42:22+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "4  2025-04-11T07:42:07+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:14,239 - INFO - Loaded PhishTank: 64320 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PhishTank Sample:\n",
      "                           text  label\n",
      "0  bayareafastrak.orgetcsw.win/      1\n",
      "1  bayareafastrak.orgetcsv.win/      1\n",
      "2  bayareafastrak.orgetcst.win/      1\n",
      "3  bayareafastrak.orgetcsr.win/      1\n",
      "4  bayareafastrak.orgetcsq.win/      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:14,836 - INFO - Loaded UCI: 5849 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UCI Sample:\n",
      "   having_IP_Address  URL_Length  Shortining_Service  having_At_Symbol  \\\n",
      "0                 -1           1                   1                 1   \n",
      "1                  1           1                   1                 1   \n",
      "2                  1           0                   1                 1   \n",
      "3                  1           0                   1                 1   \n",
      "4                  1           0                  -1                 1   \n",
      "\n",
      "   double_slash_redirecting  Prefix_Suffix  having_Sub_Domain  SSLfinal_State  \\\n",
      "0                        -1             -1                 -1              -1   \n",
      "1                         1             -1                  0               1   \n",
      "2                         1             -1                 -1              -1   \n",
      "3                         1             -1                 -1              -1   \n",
      "4                         1             -1                  1               1   \n",
      "\n",
      "   Domain_registeration_length  Favicon  ...  popUpWidnow  Iframe  \\\n",
      "0                           -1        1  ...            1       1   \n",
      "1                           -1        1  ...            1       1   \n",
      "2                           -1        1  ...            1       1   \n",
      "3                            1        1  ...            1       1   \n",
      "4                           -1        1  ...           -1       1   \n",
      "\n",
      "   age_of_domain  DNSRecord  web_traffic  Page_Rank  Google_Index  \\\n",
      "0             -1         -1           -1         -1             1   \n",
      "1             -1         -1            0         -1             1   \n",
      "2              1         -1            1         -1             1   \n",
      "3             -1         -1            1         -1             1   \n",
      "4             -1         -1            0         -1             1   \n",
      "\n",
      "   Links_pointing_to_page  Statistical_report  label  \n",
      "0                       1                  -1      1  \n",
      "1                       1                   1      1  \n",
      "2                       0                  -1      1  \n",
      "3                      -1                   1      1  \n",
      "4                       1                   1      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Enron\n",
    "try:\n",
    "    enron_path = 'data/enron_spam_data.csv'\n",
    "    if not os.path.exists(enron_path):\n",
    "        raise FileNotFoundError(f\"{enron_path} not found\")\n",
    "    enron_df = pd.read_csv(enron_path, encoding='latin1')\n",
    "    logger.info(f\"Raw Enron Columns: {enron_df.columns.tolist()}\")\n",
    "    print(\"Raw Enron Head:\\n\", enron_df.head())\n",
    "    print(\"Raw Enron Missing Values:\\n\", enron_df.isnull().sum())\n",
    "    print(\"Raw Spam/Ham Unique Values:\\n\", enron_df['Spam/Ham'].unique())\n",
    "\n",
    "    expected_columns = ['Message ID', 'Subject', 'Message', 'Spam/Ham', 'Date']\n",
    "    if not all(col in enron_df.columns for col in ['Message', 'Spam/Ham']):\n",
    "        raise ValueError(f\"Enron CSV missing required columns: {['Message', 'Spam/Ham']}\")\n",
    "    # Combine Subject and Message, handling missing values\n",
    "    enron_df['text_input'] = enron_df['Subject'].fillna('') + ' ' + enron_df['Message'].fillna('')\n",
    "    enron_df['text'] = enron_df['text_input'].apply(lambda x: clean_text(x, is_url=False) if x.strip() else '')\n",
    "    enron_df['label'] = enron_df['Spam/Ham'].map({'spam': 1, 'ham': 0, 'Spam': 1, 'Ham': 0})\n",
    "    logger.info(f\"Empty Text Count: {(enron_df['text'] == '').sum()}\")\n",
    "    logger.info(f\"Label Missing Values: {enron_df['label'].isnull().sum()}\")\n",
    "    enron_df = enron_df[['text', 'label']].dropna(subset=['label'])\n",
    "    enron_df = enron_df[enron_df['text'].str.strip() != '']  # Drop empty texts\n",
    "    enron_df.to_csv('data/intermediate/enron_processed.csv', index=False)\n",
    "    logger.info(f\"Loaded Enron: {enron_df.shape[0]} rows\")\n",
    "    print(\"Enron Sample:\")\n",
    "    print(enron_df.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading Enron: {e}\")\n",
    "    enron_df = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# Load PhishTank\n",
    "try:\n",
    "    phishtank_path = 'data/phishtank_data.csv'\n",
    "    if not os.path.exists(phishtank_path):\n",
    "        raise FileNotFoundError(f\"{phishtank_path} not found\")\n",
    "    phishtank_df = pd.read_csv(phishtank_path, encoding='latin1')\n",
    "    if 'url' not in phishtank_df.columns:\n",
    "        raise ValueError(\"PhishTank CSV missing 'url' column\")\n",
    "    print(\"Raw PhishTank Head:\\n\", phishtank_df.head())\n",
    "    phishtank_df['text'] = phishtank_df['url'].apply(lambda x: clean_text(x, is_url=True) if isinstance(x, str) else '')\n",
    "    phishtank_df['label'] = 1\n",
    "    phishtank_df = phishtank_df[['text', 'label']].dropna(subset=['text', 'label'])\n",
    "    phishtank_df = phishtank_df.drop_duplicates(subset=['text'])\n",
    "    phishtank_df.to_csv('data/intermediate/phishtank_processed.csv', index=False)\n",
    "    logger.info(f\"Loaded PhishTank: {phishtank_df.shape[0]} rows\")\n",
    "    print(\"\\nPhishTank Sample:\")\n",
    "    print(phishtank_df.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading PhishTank: {e}\")\n",
    "    phishtank_df = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# Load UCI\n",
    "try:\n",
    "    uci_path = 'data/Training_Dataset.arff'\n",
    "    if not os.path.exists(uci_path):\n",
    "        raise FileNotFoundError(f\"{uci_path} not found\")\n",
    "    uci_data, _ = arff.loadarff(uci_path)\n",
    "    uci_df = pd.DataFrame(uci_data)\n",
    "    if 'Result' not in uci_df.columns:\n",
    "        raise ValueError(\"UCI ARFF missing 'Result' column\")\n",
    "    for col in uci_df.columns:\n",
    "        uci_df[col] = uci_df[col].apply(lambda x: int(x.decode('utf-8')) if isinstance(x, bytes) else x)\n",
    "    uci_df['label'] = uci_df['Result'].apply(lambda x: 1 if x == -1 else 0)\n",
    "    uci_df = uci_df.drop(columns=['Result']).dropna()\n",
    "    uci_df = uci_df.drop_duplicates()\n",
    "    uci_df.to_csv('data/intermediate/uci_processed.csv', index=False)\n",
    "    logger.info(f\"Loaded UCI: {uci_df.shape[0]} rows\")\n",
    "    print(\"\\nUCI Sample:\")\n",
    "    print(uci_df.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading UCI: {e}\")\n",
    "    uci_df = pd.DataFrame(columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522aece3",
   "metadata": {},
   "source": [
    "## Dataset Summaries\n",
    "\n",
    "Analyze size, columns, missing values, and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d835fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:15,037 - INFO - Enron Summary: {'Size': 33652, 'Columns': ['text', 'label'], 'Missing Values': {'text': 0, 'label': 0}, 'Duplicates': np.int64(4692)}\n",
      "2025-05-09 19:14:15,077 - INFO - PhishTank Summary: {'Size': 64320, 'Columns': ['text', 'label'], 'Missing Values': {'text': 0, 'label': 0}, 'Duplicates': np.int64(0)}\n",
      "2025-05-09 19:14:15,086 - INFO - UCI Summary: {'Size': 5849, 'Columns': ['having_IP_Address', 'URL_Length', 'Shortining_Service', 'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix', 'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length', 'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor', 'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL', 'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe', 'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank', 'Google_Index', 'Links_pointing_to_page', 'Statistical_report', 'label'], 'Missing Values': {'having_IP_Address': 0, 'URL_Length': 0, 'Shortining_Service': 0, 'having_At_Symbol': 0, 'double_slash_redirecting': 0, 'Prefix_Suffix': 0, 'having_Sub_Domain': 0, 'SSLfinal_State': 0, 'Domain_registeration_length': 0, 'Favicon': 0, 'port': 0, 'HTTPS_token': 0, 'Request_URL': 0, 'URL_of_Anchor': 0, 'Links_in_tags': 0, 'SFH': 0, 'Submitting_to_email': 0, 'Abnormal_URL': 0, 'Redirect': 0, 'on_mouseover': 0, 'RightClick': 0, 'popUpWidnow': 0, 'Iframe': 0, 'age_of_domain': 0, 'DNSRecord': 0, 'web_traffic': 0, 'Page_Rank': 0, 'Google_Index': 0, 'Links_pointing_to_page': 0, 'Statistical_report': 0, 'label': 0}, 'Duplicates': np.int64(0)}\n",
      "2025-05-09 19:14:15,097 - INFO - No missing values in Enron\n",
      "2025-05-09 19:14:15,106 - INFO - No missing values in PhishTank\n",
      "2025-05-09 19:14:15,106 - INFO - No missing values in UCI\n"
     ]
    }
   ],
   "source": [
    "summaries = {}\n",
    "summaries['Enron'] = dataset_summary(enron_df, 'Enron')\n",
    "summaries['PhishTank'] = dataset_summary(phishtank_df, 'PhishTank')\n",
    "summaries['UCI'] = dataset_summary(uci_df, 'UCI')\n",
    "\n",
    "# Plot missing values\n",
    "plot_missing_values(enron_df, 'Enron')\n",
    "plot_missing_values(phishtank_df, 'PhishTank')\n",
    "plot_missing_values(uci_df, 'UCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e8ea5",
   "metadata": {},
   "source": [
    "## Label Distribution\n",
    "\n",
    "Examine phishing (1) vs. legitimate (0) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3bc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:15,246 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:15,266 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:15,531 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:15,540 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:15,716 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:15,727 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    }
   ],
   "source": [
    "label_distributions = {}\n",
    "label_distributions['Enron'] = label_distribution(enron_df, 'Enron')\n",
    "label_distributions['PhishTank'] = label_distribution(phishtank_df, 'PhishTank')\n",
    "label_distributions['UCI'] = label_distribution(uci_df, 'UCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4e42f",
   "metadata": {},
   "source": [
    "## Text Analysis (Enron and PhishTank)\n",
    "\n",
    "Analyze text length, word frequency, n-grams, clusters, and word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d10888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:16,007 - INFO - Enron Text Stats: {'Char Length Mean': np.float64(1092.2825983596815), 'Char Length Median': np.float64(491.0), 'Word Length Mean': np.float64(1.0), 'Word Length Median': np.float64(1.0)}\n",
      "C:\\Users\\Akoba\\AppData\\Local\\Temp\\ipykernel_7052\\1689177729.py:122: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "2025-05-09 19:14:26,107 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:26,147 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:26,413 - INFO - PhishTank Text Stats: {'Char Length Mean': np.float64(45.44395211442786), 'Char Length Median': np.float64(26.0), 'Word Length Mean': np.float64(1.0012748756218905), 'Word Length Median': np.float64(1.0)}\n",
      "C:\\Users\\Akoba\\AppData\\Local\\Temp\\ipykernel_7052\\1689177729.py:162: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "2025-05-09 19:14:33,596 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:33,658 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    }
   ],
   "source": [
    "text_stats = {}\n",
    "text_stats['Enron'] = text_analysis(enron_df, 'Enron')\n",
    "text_stats['PhishTank'] = text_analysis(phishtank_df, 'PhishTank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be257c62",
   "metadata": {},
   "source": [
    "## UCI Feature Analysis\n",
    "\n",
    "Analyze numerical features and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd6c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:33,916 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:33,942 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:34,294 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:34,306 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:34,506 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:34,525 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:34,700 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:34,716 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,007 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,026 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,266 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,282 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,477 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,495 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,707 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,723 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,930 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:35,946 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    }
   ],
   "source": [
    "uci_stats, uci_corr, uci_importance = uci_feature_analysis(uci_df, 'UCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439a68f",
   "metadata": {},
   "source": [
    "## Combined Dataset Analysis\n",
    "\n",
    "Combine Enron and PhishTank for text-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e8d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:14:38,616 - INFO - Combined Summary: {'Size': 97972, 'Columns': ['text', 'label'], 'Missing Values': {'text': 0, 'label': 0}, 'Duplicates': np.int64(4692)}\n",
      "2025-05-09 19:14:38,636 - INFO - No missing values in Combined\n",
      "2025-05-09 19:14:38,667 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:14:38,696 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:15:05,748 - INFO - Balanced Summary: {'Size': 162854, 'Columns': ['label'], 'Missing Values': {'label': 0}, 'Duplicates': np.int64(162852)}\n",
      "2025-05-09 19:15:05,936 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:15:05,966 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:15:06,281 - INFO - Applied SMOTE to balance combined dataset\n",
      "2025-05-09 19:15:06,856 - INFO - Combined Text Stats: {'Char Length Mean': np.float64(405.0182603192749), 'Char Length Median': np.float64(43.0), 'Word Length Mean': np.float64(1.0008369738292573), 'Word Length Median': np.float64(1.0)}\n",
      "C:\\Users\\Akoba\\AppData\\Local\\Temp\\ipykernel_7052\\1689177729.py:122: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Akoba\\AppData\\Local\\Temp\\ipykernel_7052\\1689177729.py:162: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "2025-05-09 19:15:24,222 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:15:24,400 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-05-09 19:15:27,072 - INFO - Saved combined dataset to data/processed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    combined_df = pd.concat([enron_df[['text', 'label']], phishtank_df[['text', 'label']]], ignore_index=True)\n",
    "    combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "    if combined_df.empty:\n",
    "        logger.warning(\"Combined dataset is empty after filtering empty text\")\n",
    "    combined_summary = dataset_summary(combined_df, 'Combined')\n",
    "    plot_missing_values(combined_df, 'Combined')\n",
    "    combined_labels = label_distribution(combined_df, 'Combined')\n",
    "\n",
    "    # Apply SMOTE for balance\n",
    "    if not combined_df.empty and combined_df['label'].nunique() > 1:\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        X_text = vectorizer.fit_transform(combined_df['text'])\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X_text, combined_df['label'])\n",
    "        balanced_df = pd.DataFrame({'label': y_res})\n",
    "        balanced_summary = dataset_summary(balanced_df, 'Balanced')\n",
    "        balanced_labels = label_distribution(balanced_df, 'Balanced')\n",
    "        logger.info(\"Applied SMOTE to balance combined dataset\")\n",
    "\n",
    "    combined_stats, combined_phishing_freq, combined_legit_freq, combined_phishing_bigrams, combined_legit_bigrams, combined_phishing_trigrams, combined_legit_trigrams = text_analysis(combined_df, 'Combined')\n",
    "\n",
    "    # Save combined dataset\n",
    "    combined_df.to_csv('data/processed_dataset.csv', index=False)\n",
    "    logger.info(\"Saved combined dataset to data/processed_dataset.csv\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in combined dataset analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54841e4",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "- **Enron**: ~33,716 emails, ~50% spam/ham, 371 missing `Message` values. Long texts (mean ~500 chars). Key features: `Message`, `Subject`, bigrams/trigrams (e.g., “click here”, “verify account”).\n",
    "- **PhishTank**: ~64,391 URLs post-duplicate removal, 100% phishing. Short texts (mean ~46 chars). Key features: `url`, bigrams (e.g., “org-etcsw”).\n",
    "- **UCI**: ~5,785 records post-duplicate removal, balanced labels. Key features: `SSLfinal_State`, `URL_of_Anchor`, `web_traffic`, `having_IP_Address`.\n",
    "- **Combined**: ~98,000 rows (Enron + PhishTank), imbalanced but balanced with SMOTE. Diverse text lengths.\n",
    "\n",
    "### Preprocessing Recommendations\n",
    "- **Enron**: Drop rows with missing `label` or empty `text`. Truncate to 512 tokens for BERT.\n",
    "- **PhishTank**: Retain URL components. Consider subsampling to reduce size.\n",
    "- **UCI**: Use numerical features for Random Forest baseline. Drop duplicates.\n",
    "- **Combined**: Use SMOTE with 1:1 ratio. Standardize cleaning. Save balanced dataset.\n",
    "\n",
    "### Next Steps\n",
    "- Verify Enron (~33,716 rows) and UCI (~5,785 rows) loading.\n",
    "- Train BERT on `processed_dataset.csv`.\n",
    "- Compare with UCI Random Forest baseline.\n",
    "- Explore text clusters for phishing campaign insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570d5154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:15:27,172 - INFO - EDA completed. Results saved to results/eda/\n"
     ]
    }
   ],
   "source": [
    "# Save EDA summary\n",
    "try:\n",
    "    with open('results/eda/summary.md', 'w') as f:\n",
    "        f.write(\"# EDA Summary\\n\\n\")\n",
    "        for name in ['Enron', 'PhishTank', 'UCI']:\n",
    "            f.write(f\"## {name}\\n\")\n",
    "            f.write(f\"**Summary**: {summaries.get(name, {})}\\n\")\n",
    "            f.write(f\"**Label Distribution**: {label_distributions.get(name, {})}\\n\")\n",
    "            if name != 'UCI':\n",
    "                stats, words_p, words_l, bigrams_p, bigrams_l, trigrams_p, trigrams_l = text_stats.get(name, [{}, [], [], [], [], [], []])\n",
    "                f.write(f\"**Text Stats**: {stats}\\n\")\n",
    "                f.write(f\"**Top Phishing Words**: {words_p}\\n\")\n",
    "                f.write(f\"**Top Legitimate Words**: {words_l}\\n\")\n",
    "                f.write(f\"**Top Phishing Bigrams**: {bigrams_p}\\n\")\n",
    "                f.write(f\"**Top Legitimate Bigrams**: {bigrams_l}\\n\")\n",
    "                f.write(f\"**Top Phishing Trigrams**: {trigrams_p}\\n\")\n",
    "                f.write(f\"**Top Legitimate Trigrams**: {trigrams_l}\\n\")\n",
    "            else:\n",
    "                f.write(f\"**Feature Stats**: {uci_stats}\\n\")\n",
    "                f.write(f\"**Feature Correlations**: \\n{uci_corr.to_string()}\\n\")\n",
    "                f.write(f\"**Feature Importance**: \\n{uci_importance.to_string()}\\n\")\n",
    "        f.write(\"## Combined (Enron + PhishTank)\\n\")\n",
    "        f.write(f\"**Summary**: {combined_summary}\\n\")\n",
    "        f.write(f\"**Label Distribution**: {combined_labels}\\n\")\n",
    "        f.write(f\"**Text Stats**: {combined_stats}\\n\")\n",
    "        f.write(f\"**Top Phishing Words**: {combined_phishing_freq}\\n\")\n",
    "        f.write(f\"**Top Legitimate Words**: {combined_legit_freq}\\n\")\n",
    "        f.write(f\"**Top Phishing Bigrams**: {combined_phishing_bigrams}\\n\")\n",
    "        f.write(f\"**Top Legitimate Bigrams**: {combined_legit_bigrams}\\n\")\n",
    "        f.write(f\"**Top Phishing Trigrams**: {combined_phishing_trigrams}\\n\")\n",
    "        f.write(f\"**Top Legitimate Trigrams**: {combined_legit_trigrams}\\n\")\n",
    "        if 'balanced_summary' in locals():\n",
    "            f.write(\"## Balanced (SMOTE)\\n\")\n",
    "            f.write(f\"**Summary**: {balanced_summary}\\n\")\n",
    "            f.write(f\"**Label Distribution**: {balanced_labels}\\n\")\n",
    "\n",
    "    logger.info(\"EDA completed. Results saved to results/eda/\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving EDA summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
